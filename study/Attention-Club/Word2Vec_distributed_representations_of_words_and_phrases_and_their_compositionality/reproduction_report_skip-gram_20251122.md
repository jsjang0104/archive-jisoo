# Literature Reproduction Report 2025.11.21

- **Title**: "Distributed Representations of Words and Phrases and their Compositionality"
- **Authors**: Tomas Mikolov et al.
- **Journal**: "arXiv:1310.4546"
- **year**: 2013

## 1. ì‹¤í—˜ ëª©ì 
1. ì§€ë‚œë²ˆ CBOW êµ¬ì¡°ì˜ word2vec model êµ¬í˜„ reportì—ì„œ ìœ ì˜ë¯¸í•œ ê²°ê³¼ë¥¼ ì°¾ì§€ ëª»í–ˆìŒìœ¼ë¡œ, word analogy testì™€ cosine similarity testì—ì„œ ì‹¤ì œ ë…¼ë¬¸ ì‹¤í—˜ ê²°ê³¼ì™€ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ë‚´ëŠ” ê²ƒì„ ëª©ì ìœ¼ë¡œí•¨

2. ì§€ë‚œë²ˆ reportì—ì„œì˜ ìµœìŠ¹íƒ êµìˆ˜ë‹˜ í”¼ë“œë°± ë°˜ì˜
  - cosine similarityê°€ ì‹¤í—˜ì˜ ì„±ê³µì˜ ì²™ë„ë¥¼ ë°˜ì˜í•˜ëŠ” ê±´ ì•„ë‹˜ -> ë‹¤ë¥¸ í‰ê°€ ë°©ì‹ ë„ì…í•˜ì˜€ìŒ (word analogy test, cosine similarity testê¸°ë°˜ ìœ ì‚¬ ë‹¨ì–´ query)
  - training loss ê¸°ë¡ í•„ìš” -> ì‹œê°í™” ì½”ë“œ ì¶”ê°€
  - naive whitespace tokenization(ì˜ˆì „ ë°©ì‹) ë³´ë‹¤ recent tokenizer from LLMs ì‚¬ìš© -> BPE ì‚¬ìš© ì‹œë„ (Experiment I only)

3. ì§€ë‚œë²ˆ reportì—ì„œì˜ Attention í•™íšŒì›ë“¤ í”¼ë“œë°± ë°˜ì˜
  - colab GPU ì‚¬ìš© ì¶”ì²œ -> local CPU ì‚¬ìš© ê³ ì§‘ì„ ë²„ë¦¬ê³  google colabì—ì„œ ë¬´ë£Œ GPU ì‚¬ìš©
  - ì‹¤í—˜ ì‹œí–‰ì°©ì˜¤ ê¸°ë¡ í˜¸í‰ -> ìœ ì§€

4. ë…¼ë¬¸ì´ ì œì‹œí•œ ë‘ ê°€ì§€ ìŸì  ì¤‘ subsamplingì€ êµ¬í˜„í•˜ì˜€ìœ¼ë‚˜, phrase í•™ìŠµì€ êµ¬í˜„í•˜ì§€ ì•ŠìŒ.

5. ì§€ë‚œë²ˆ reportì—ì„œ ì‚¬ìš©í–ˆë˜ CBOWê°€ ì•„ë‹Œ Skip-gram êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‘ê°€ì§€ ëª¨ë¸ ì½”ë“œì— ëŒ€í•´ ì°¨ì´ì ì„ í•™ìŠµí•¨.

## 2. Experiment Log
### 1. Environment
Hardware & Infrastructure
- Platform: Google Colab (Cloud Computing)
- GPU: NVIDIA Tesla T4 (VRAM 16GB)
- Local Environment (Preprocessing & Control):
  - CPU: 13th Gen IntelÂ® Coreâ„¢ i5-1340P
  - RAM: 16.0 GB
  - OS: Windows 11 Home

Software & Library
- Language: Python 3.12.12
- Deep Learning Framework: PyTorch 2.2.2
- Libraries: TorchText (Dataset), Transformers (Tokenizer), Matplotlib (Visualization), Numpy

### 2. Dataset *by Gemini*
- Source: AG_NEWS (News Classification Dataset)
- Volume: Total 127,600 Samples (Train: 120,000 / Test: 7,600)
- Characteristics: World, Sports, Business, Sci/Techì˜ 4ê°€ì§€ ì£¼ì œë¡œ êµ¬ì„±ëœ ë‰´ìŠ¤ ê¸°ì‚¬ ë°ì´í„°ë¡œ, ê³ ìœ ëª…ì‚¬ì™€ ì „ë¬¸ ìš©ì–´ê°€ ë‹¤ìˆ˜ í¬í•¨ë¨.

### 3. ëª¨ë¸ êµ¬ì¡° ë° íŠ¹ì´ì‚¬í•­
- **ëª¨ë¸**: Skip-gram with negative sampling
- ìµœì í™” ë°©ë²•:
  - Subsampling: Frequent Wordsì˜ í•™ìŠµ í™•ë¥ ì„ ë‚®ì¶¤
  - Dynamic Window: ìœˆë„ìš° í¬ê¸°ë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì ˆí•˜ì—¬ ì¤‘ì‹¬ ë‹¨ì–´ì— ê°€ì¤‘ì¹˜ ë¶€ì—¬ *by Gemini*
- ìê¾¸ importí•´ë‘” ê²Œ ë‚ ì•„ê°€ê³ , ìºì‹œê°€ ì¶©ëŒí•˜ëŠ” ìƒí™©ì´ ë°œìƒí•˜ì—¬ì„œ ì‹¤í—˜ í•˜ë‚˜ë‹¹ í•˜ë‚˜ì˜ cell ì•ˆì— ëª¨ë‘ êµ¬í˜„í•˜ì˜€ìŒ. (í•˜ë‚˜ì˜ ì‹¤í—˜ì„ ì§„í–‰í•  ë•Œ ë§ˆë‹¤ ëŸ°íƒ€ì„ì„ ëª¨ë‘ ì´ˆê¸°í™”)
- ì½”ë“œ ìƒì„± ì „ë°˜ì— ì œë¯¸ë‚˜ì´ì˜ ë„ì›€ì„ ë°›ì•˜ìŒ (ë”°ë¡œ ëª…ì‹œx)
- text ìƒì„± ì¼ë¶€ì— ìˆì–´ ì œë¯¸ë‚˜ì´ì˜ ë„ì›€ì„ ë°›ì•˜ìŒ (ëª…ì‹œo (*by Gemini*))

## 3. Experiment I

### 3-1. Hyperparameter
- ì„ë² ë”© ì°¨ì› = 100
- window sizse = 5
- ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§ = 5
- learning rate = 0.003
- ë°°ì¹˜ ì‚¬ì´ì¦ˆ = 1024 # batch size
- ì—í¬í¬ = 5
- minimun frequency = 5

### 3-2. Tokenization
Byte-Pair-Encoding: recent tokenizer from LLMì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ BPE(GPT-2 Tokenizer) ì‹œë„.
- í˜•íƒœì†Œ ë¶„ì„ì— ê°•í•¨.
- ì˜ˆ: apples can be decomposed into apple and -s

### 3-3. Result

1. Training Loss ê²½í–¥: training lossê°€ properlyí•˜ê²Œ decreaseí•˜ëŠ” ì¤‘
![image.png](attachment:image.png)

2. Cosine Similarity ê³„ì‚°ì„ í†µí•œ ìœ ì‚¬ ë‹¨ì–´ ì§ˆë¬¸ğŸ˜­
- [Query] microsoft
  - Ä micro: 0.6243
  - Ä authentication: 0.6126
  - Ä exploit: 0.5390
  - think: 0.5385
  - Ä innovation: 0.5357

- [Query] football
  - Ä substituted: 0.4875
  - Ä scrimmage: 0.4691
  - Ä stale: 0.4601
  - Ä match: 0.4356
  - Ä jumping: 0.4349

- [Query] president
  - Ä president: 0.5021
  - Ä choosing: 0.4810
  - idate: 0.4756
  - Ä unrealistic: 0.4480
  - Ä vice: 0.4424

- [Query] war
  - rahim: 0.5435
  - enegger: 0.5222
  - Ä traitor: 0.4855
  - Ä leftist: 0.4822
  - Ä convened: 0.4795

3. Word Analogy Test ğŸ˜­
- [Analogy] man:king = woman:?
  - inic (0.4361)
  - Ä donation (0.4071)
  - Ä lifetime (0.4040)

- [Analogy] bush:president = jobs:?
  - direction (0.4057)
  - Ä reorgan (0.4042)
  - Ä payment (0.3920)

### 3-4. Discussion
1. **ê²°ê³¼ ì¶œë ¥ í˜•íƒœê°€ ì´ìƒí•¨**: BPEë¥¼ í†µí•´ ì²˜ë¦¬í•œ Tokenì˜ í˜•íƒœê°€ ê·¸ëŒ€ë¡œ ì¶œë ¥ (ê³µë°± í‘œí˜„ Ä  ë“±). ì´ë¥¼ ë‹¤ì‹œ ì‚¬ëŒì´ ì•Œì•„ë³¼ ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë°”ê¿”ì£¼ëŠ” ì½”ë“œì˜ ë¶€ì¬ ë•Œë¬¸ì„

2. **ë‹¨ì–´ê°€ ì•„ë‹Œ í˜•íƒœì†Œì /ì² ìì  íŒŒí¸ í˜•íƒœ ì¶œë ¥**: ì´ëŠ” ì–•ì€ ì‹ ê²½ë§ êµ¬ì¡°ë¥¼ ê°–ê³  ìˆëŠ” Word2Vec ëª¨ë¸ êµ¬ì¡°ìƒ, ìª¼ê°œì§„ ì„œë¸Œì›Œë“œ(Subword) ë²¡í„°ë“¤ì„ ë¬¸ë§¥ì ìœ¼ë¡œ ë‹¤ì‹œ ê²°í•©(Re-composition)í•˜ì—¬ í•˜ë‚˜ì˜ ì˜ë¯¸ë¡œ í†µí•©í•˜ëŠ” Attention ë©”ì»¤ë‹ˆì¦˜ì´ ë¶€ì¬í•˜ê¸° ë•Œë¬¸ *by Gemini*

3. **Evaluation ê²°ê³¼ê°€ ìœ ì˜ë¯¸í•˜ì§€ ì•ŠìŒ**: 2ë²ˆê³¼ ê°™ì€ ì›ì¸ì—ì„œ ê¸°ì¸í•œ ê²ƒìœ¼ë¡œ ë³´ì„.

1,2,3ë¡œë¶€í„° Tokenizationê¸°ë²•ì„ BPEì—ì„œ **Basic English Tokenizer**ë¡œ ë³€ê²½ (Naive White Spaceë³´ë‹¤ ë” ì •í™•: ì†Œë¬¸ìí™” + êµ¬ë‘ì (, . ! ë“±)ì„ ë‹¨ì–´ì—ì„œ ë–¼ì–´ëƒ„)

4. GPU ì‚¬ìš©ìœ¼ë¡œ í•™ìŠµì‹œê°„ì´ ì˜ˆìƒë³´ë‹¤ ë§ì´ ì§§ì•„ ì¢€ ë” ë‚˜ì€ í’ˆì§ˆì„ ìœ„í•˜ì—¬ **hyperparameter ì¡°ì •**
- window size: 5 -> 10
- epoch: 5 -> 15
- minimun frequency: 5 -> 10

## 4. Experiment II
### 4-1. Hyperparameter
- ì„ë² ë”© ì°¨ì› = 100
- window sizse = 10
- ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§ = 5
- learning rate = 0.003
- ë°°ì¹˜ ì‚¬ì´ì¦ˆ = 1024 # batch size
- ì—í¬í¬ = 15
- minimun frequency = 10

### 4-2. Tokenization
Basic English Tokenizer: Normalization (ì†Œë¬¸ìí™”) -> Punctuation Splitting (êµ¬ë‘ì  ë¶„ë¦¬)
- ë‹¨ì–´ ë‹¨ìœ„ í† í°í™”
- ì˜ˆ: Hi! -> hi

### 4-3. Result
1. Training Loss ê²½í–¥: training lossê°€ properlyí•˜ê²Œ decreaseí•˜ëŠ” ì¤‘
![image.png](attachment:image.png)

2. Cosine Similarity ê³„ì‚°ì„ í†µí•œ ìœ ì‚¬ ë‹¨ì–´ ì§ˆë¬¸
- [Query] microsoft ğŸ˜„
  - software: 0.6035
  - server: 0.5773
  - msft: 0.5548
  - vulnerabilities: 0.5541
  - pack: 0.5462

- [Query] football ğŸ˜„
  - chester: 0.5503
  - coach: 0.5412
  - smith: 0.5352
  - micky: 0.5026
  - players: 0.4976

- [Query] president ğŸ˜„
  - vice: 0.5408
  - bush: 0.5064
  - ally: 0.5041
  - emphasize: 0.4924
  - elected: 0.4857

- [Query] war ğŸ˜„
  - sarajevo: 0.4962
  - favoring: 0.4874
  - envoy: 0.4837
  - yugoslavia: 0.4811
  - sudans: 0.4630

3. Word Analogy Test
- [Analogy] man:king = woman:? ğŸ˜­
  - dancer (0.5620)
  - norodom (0.5264)
  - cambodian (0.5080)

- [Analogy] bush:president = jobs:? ğŸ˜­
  - bombardier (0.4913)
  - investor (0.4418)
  - workforce (0.4352)

### 4-4. Discussion
1. Word Analogyì˜ ì‹¤íŒ¨: **ë°ì´í„°ì…‹ í¸í–¥** *by Gemini*
- [Analogy] bush : president = jobs : ? ì—ì„œ apple, ceoë¥¼ ê¸°ëŒ€í–ˆì§€ë§Œ ëª¨ë¸ ì˜ˆì¸¡ì€ workforce, investorì„
  - AG_NEWS ë‰´ìŠ¤ ë°ì´í„°ëŠ” 2004ë…„ ìë£Œì´ê³  ë‹¹ì‹œ "Jobs"ëŠ” ìŠ¤í‹°ë¸Œ ì¡ìŠ¤ë³´ë‹¤ "ì¼ìë¦¬(Employment/Economy)"ë¼ëŠ” ì˜ë¯¸ë¡œ í›¨ì”¬ ë§ì´ ì“°ì˜€ìŒ.
  - í•´ì„: bushê°€ presidentìœ¼ë¡œì„œ ì •ì±…ì„ ë‹¤ë£¨ë“¯, jobs(ì¼ìë¦¬)ëŠ” ë…¸ë™ë ¥(workforce)ì´ë‚˜ ê²½ì œì™€ ê´€ë ¨ì´ ìˆë‹¤ëŠ” ë¬¸ë§¥ì„ í•™ìŠµ.

- [Analogy] man : king = woman : ? ì—ì„œ queenì„ ê¸°ëŒ€í–ˆì§€ë§Œ ëª¨ë¸ ì˜ˆì¸¡ì€ norodom (ë…¸ë¡œë” ì‹œì•„ëˆ„í¬ ìº„ë³´ë””ì•„ êµ­ì™•(?))ì„
  - ë‰´ìŠ¤ ë°ì´í„° íŠ¹ì„±ìƒ "ì˜›ë‚  ì´ì•¼ê¸° ì†ì˜ King"ë³´ë‹¤ëŠ” "ì‹¤ì¡´ ì¸ë¬¼ ìº„ë³´ë””ì•„ êµ­ì™• ë…¸ë¡œë”"ì— ëŒ€í•œ ê¸°ì‚¬ê°€ ë§ìŒ
  - í•´ì„: Kingì´ë¼ëŠ” ë‹¨ì–´ì˜ ë²¡í„°ê°€ 'ë‚¨ì„± í†µì¹˜ì'ë¼ëŠ” ì¶”ìƒì  ê°œë…ë³´ë‹¤ëŠ” 'ë…¸ë¡œë”'ì´ë¼ëŠ” ê³ ìœ ëª…ì‚¬ ì˜†ì— í˜•ì„±ë¨

2. Cosine Similarity ê¸°ë°˜ Queryì˜ ì„±ê³µ
- Cosine SimilarityëŠ” ë²¡í„° ê³µê°„ ë‚´ ìœ ì‚¬í•œ ë¶€ë¶„ì— ë­‰ì³ìˆëŠ” ê²ƒì— ëŒ€í•œ ì¸¡ì •ì´ê¸° ë•Œë¬¸ì— ì •êµí•œ ì—°ì‚°ì´ í•„ìš” ì—†ìŒ (Microsoft - Software ë“± ìš°ìˆ˜í•œ evaluation í’ˆì§ˆì„ ë³´ì„)
- AnalogyëŠ” ë²¡í„° ê³µê°„ ë‚´ì—ì„œ ì •êµí•œ í‰í–‰ì‚¬ë³€í˜•ì„ ê·¸ë ¤ì•¼ í•˜ë¯€ë¡œ ë…¼ë¬¸ ìˆ˜ì¤€ìœ¼ë¡œ í•™ìŠµì´ í›¨ì”¬ ë§ì´ ì´ë£¨ì–´ì ¸ì•¼í•¨

3. **Learning Rate Scheduler** ì¶”ê°€: í•™ìŠµ í›„ë°˜ë¶€ì— í•™ìŠµë¥ ì„ ì¤„ì—¬ì¤˜ì•¼ ë” ì •êµí•œ ë²¡í„° ê³µê°„ ìƒì„± ê°€ëŠ¥ *by Gemini*
- ë†’ì€ í•™ìŠµë¥ ì„ ê³ ì •í•´ì„œ ìœ ì§€í•  ê²½ìš°, í•™ìŠµ í›„ë°˜ë¶€ì— Global Minimum ê·¼ì²˜ì— ë„ë‹¬í•˜ë”ë¼ë„ ìˆ˜ë ´í•˜ì§€ ëª»í•˜ê³  ì£¼ë³€ì„ ë§´ë„ëŠ” Oscillation í˜„ìƒì´ ë°œìƒí•  ìˆ˜ ìˆìŒ
- ëŒ€ì‹  ì´ˆë°˜ learning rateë¥¼ í¬ê²Œ ìˆ˜ì • 0.003 -> 0.025

## 5. Experiment III
### 5-1. Hyperparameter
- ì„ë² ë”© ì°¨ì› = 100
- window sizse = 10
- ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§ = 5
- learning rate = 0.025
- ë°°ì¹˜ ì‚¬ì´ì¦ˆ = 1024 # batch size
- ì—í¬í¬ = 15
- minimun frequency = 10

### 5-2. Tokenization
Basic English Tokenizer: Normalization (ì†Œë¬¸ìí™”) -> Punctuation Splitting (êµ¬ë‘ì  ë¶„ë¦¬)
- ë‹¨ì–´ ë‹¨ìœ„ í† í°í™”
- ì˜ˆ: Hi! -> hi

### 5-3. Result
1. Training Loss ê²½í–¥:

- ì´ˆê¸° ì˜¤ë²„ìŠˆíŒ…: Epoch 0ì—ì„œ 1ë¡œ ë„˜ì–´ê°€ëŠ” ì‹œì ì— Lossê°€ ì˜¤ë²„ìŠˆíŒ…ì´ ë°œìƒí•¨. ì´ëŠ” Adam Optimizerì˜ ê´€ì„±(Momentum)ê³¼ ì´ˆê¸° ë†’ì€ í•™ìŠµë¥ (High Learning Rate)ë¡œ ì¸í•´ ìµœì ì  íƒìƒ‰ ê³¼ì •ì—ì„œ ë°œìƒí•œ ì¼ì‹œì ì¸ í˜„ìƒìœ¼ë¡œ ë¶„ì„ë¨. *by Gemini*
- ì•ˆì •ì  ìˆ˜ë ´ (Stable Convergence): Epoch 2 ì´í›„ë¶€í„°ëŠ” ë¶€ë“œëŸ½ê²Œ ê°ì†Œí•¨.

ìµœì¢… ê²€ì¦ì„ ìœ„í•˜ì—¬ Experiment IIIì—ì„œë§Œ ì¶”ê°€ ì‹¤í—˜ì„ ì§„í–‰í•˜ì˜€ìŒ

2. Cosine Similarity ê³„ì‚°ì„ í†µí•œ ìœ ì‚¬ ë‹¨ì–´ ì§ˆë¬¸
[Query] medal ğŸ˜„
  - meters: 0.6535
  - athens: 0.6438
  - freestyle: 0.6392
  - olympic: 0.6389
  - phelps: 0.5901

[Query] cpu ğŸ˜„
  - xserve: 0.4809
  - usability: 0.4641
  - cordless: 0.4418
  - gran: 0.4365
  - 4-gigahertz: 0.4345

[Query] gold ğŸ˜„
  - olympic: 0.5785
  - olympics: 0.5728
  - medal: 0.5696
  - silver: 0.5624
  - phelps: 0.5467

[Query] internet ğŸ˜„
  - users: 0.6179
  - customers: 0.5760
  - web: 0.5671
  - search: 0.5439
  - computer: 0.5437

[Query] microsoft ğŸ˜„
  - xp: 0.6307
  - windows: 0.6283
  - software: 0.6008
  - users: 0.5970
  - designed: 0.5897

[Query] football ğŸ˜„
  - coach: 0.5445
  - soccer: 0.5405
  - dolphins: 0.5392
  - club: 0.5307
  - henry: 0.5293

[Query] president ğŸ˜„
  - minister: 0.6303
  - coalition: 0.6108
  - bush: 0.6104
  - democratic: 0.6063
  - elections: 0.6034

[Query] war ğŸ˜„
  - hiding: 0.5535
  - afp: 0.5350
  - iraq: 0.5182
  - genocide: 0.5142
  - abuses: 0.5010

3. Word Analogy Test
[Analogy] china:beijing = japan:? ğŸ˜¶
  - korea (0.4301)
  - iran (0.4273)
  - chinese (0.4263)

[Analogy] france:paris = germany:? ğŸ˜¶
  - balance (0.4186)
  - struggled (0.4010)
  - reuters (0.3945)

[Analogy] microsoft:software = intel:? ğŸ˜„
  - devices (0.5559)
  - processors (0.5432)
  - dual-core (0.5343)

[Analogy] microsoft:windows = google:? ğŸ˜„
  - search (0.5490)
  - beta (0.5095)
  - web (0.5031)

[Analogy] swimming:phelps = cycling:? ğŸ˜¶
  - gymnastics (0.4573)
  - silver (0.4239)
  - spain (0.4088)


### 5-4. Discussion
1. Cosine Similarity ê³„ì‚°ì„ í†µí•œ ìœ ì‚¬ ë‹¨ì–´ ì§ˆë¬¸: ëª¨ë“  Queryì—ì„œ ì¢‹ì€ ê²°ê³¼ë¥¼ ë‚˜íƒ€ëƒ„. Word Vector Spaceê°€ ì˜ í˜•ì„±ë˜ì—ˆìŒì„ ì•Œ ìˆ˜ ìˆìŒ

2. Word Analogy Test
- ì„±ê³µ ì‚¬ë¡€: "ê¸°ì—…"ê³¼ "í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤"ê°„ì˜ ê´€ê³„ë¥¼ ì™„ë²½í•˜ê²Œ ê³„ì‚°í•´ëƒ„!!!
  - [Analogy] microsoft:software = intel:devices
  - [Analogy] microsoft:windows = google:search
- ì‹¤íŒ¨ ì‚¬ë¡€
  - [Analogy] china:beijing = japan:korea ë‰´ìŠ¤ ë°ì´í„° íŠ¹ì„±ìƒ 'êµ­ê°€-ìˆ˜ë„'ì˜ ì§€ë¦¬ì  ì‚¬ì‹¤ë³´ë‹¤ ë™ë¶ì•„ ì™¸êµ ê´€ê³„(ì¤‘êµ­-ì¼ë³¸-í•œêµ­)ì˜ ì–¸ê¸‰ ë¹ˆë„ê°€ ì••ë„ì ìœ¼ë¡œ ë†’ì•„, ëª¨ë¸ì´ ì¼ë³¸ì˜ ì—°ê´€ ë‹¨ì–´ë¡œ ìˆ˜ë„(ë„ì¿„)ê°€ ì•„ë‹Œ ì™¸êµì  íŒŒíŠ¸ë„ˆ(í•œêµ­)ë¥¼ ì„ íƒí•¨ *by Gemini*
  - [Analogy] france:paris = germany:balance ë…ì¼ê³¼ í”„ë‘ìŠ¤ê°€ ì£¼ë¡œ ê²½ì œ/ì •ì¹˜ ë‰´ìŠ¤ì—ì„œ í•¨ê»˜ ë‹¤ë£¨ì–´ì§€ë©° 'ë¬´ì—­ ìˆ˜ì§€(Trade Balance)'ë‚˜ 'ì„¸ë ¥ ê· í˜•(Balance of Power)' ê°™ì€ ê²½ì œÂ·ì‹œì‚¬ ìš©ì–´ì™€ ê°•í•˜ê²Œ ì—°ê²°ë˜ì–´ ë²¡í„°ê°€ í˜•ì„±ë˜ì—ˆê¸° ë•Œë¬¸ì„ *by Gemini*
  - [Analogy] swimming:phelps = cycling:gymnastics ìˆ˜ì˜, ì‚¬ì´í´, ì²´ì¡°ê°€ ëª¨ë‘ 'í•˜ê³„ ì˜¬ë¦¼í”½ ì¸ê¸° ì¢…ëª©'ì´ë¼ëŠ” ì¢ê³  ë°€ì§‘ëœ í´ëŸ¬ìŠ¤í„°ë¥¼ í˜•ì„±í•˜ê³  ìˆì–´, ë²¡í„° ì—°ì‚° ê²°ê³¼ê°€ íŠ¹ì • ì„ ìˆ˜ë¥¼ ê°€ë¦¬í‚¤ì§€ ëª»í•˜ê³  ê°€ì¥ ê°€ê¹Œìš´ ë‹¤ë¥¸ ì¸ê¸° ì¢…ëª©(ì²´ì¡°)ìœ¼ë¡œ í¸í–¥ë¨ *by Gemini*

## 6. Conclusion
1. Skip-gramê³¼ ê°™ì€ ì–•ì€ ì‹ ê²½ë§ êµ¬ì¡°ì—ì„œëŠ” BPE ë°©ì‹ë³´ë‹¤ word ê¸°ë°˜ì˜ tokenization ê¸°ë²•ì´ ë” ì˜ ì‘ë™í•¨

2. adaptive learining rate ë„ì…ì˜ ì¤‘ìš”ì„±: global minimumì— ê°€ê¹Œì›Œì§ˆ ìˆ˜ë¡ learning rateë¥¼ ì ê²Œ ì¡°ì •í•´ì•¼ ì§„ë™í˜„ìƒì„ í”¼í•˜ê³  ì¢€ ë” í€„ë¦¬í‹° ë†’ì€ ë²¡í„° ê³µê°„ì„ ë§Œë“¤ì–´ ëƒ„

3. Datasetì˜ biasì— ê¸°ë°˜í•œ evaluationì„ ì§„í–‰í•´ì•¼ ì˜ë¯¸ìˆëŠ” ê²°ë¡ ì„ ë„ì¶œí•´ ë‚¼ ìˆ˜ ìˆìŒ

3. í•œê³„ ë° ë°œì „ ë°©í–¥: ë³¸ ì‹¤í—˜ì—ì„œëŠ” ì›ë³¸ ë…¼ë¬¸ì—ì„œ ë„ì¶œí•´ëƒˆë˜ ì¤‘ìš”í•œ ì˜ì˜ì¤‘ í•˜ë‚˜ì¸ multi-word expressionì´ë‚˜ phrase í•™ìŠµì„ ì§„í–‰í•˜ì§€ ëª»í•˜ì˜€ìŒ. ì›ë³¸ì—ì„œëŠ” í†µê³„ì  ê¸°ë²•(Unigram/Bigram Counts)ì„ í™œìš©í•œ ì „ì²˜ë¦¬ ë‹¨ê³„ì—ì„œ Phrasesë¥¼ ì‹ë³„í•˜ì˜€ìŒ. ì¶”í›„ ì´ë¥¼ ë„ì…í•˜ì—¬ ì¬ì‹¤í—˜í•´ë³¼ ìˆ˜ ìˆìŒ.